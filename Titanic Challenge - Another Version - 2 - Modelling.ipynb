{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "In this part, we use our knowledge of the passengers based on the features we created and then build a statistical model. \n",
    "\n",
    "There is a wide variety of models to use, from logistic regression to decision trees and more sophisticated ones such as random forests and gradient boosted trees.\n",
    "\n",
    "Back to our problem, we now have to:\n",
    "\n",
    "1. Break the combined dataset in train set and test set.\n",
    "2. Use the train set to build a predictive model.\n",
    "3. Evaluate the model using the train set.\n",
    "4. Test the model using the test set and generate and output file for the submission.\n",
    "\n",
    "Keep in mind that we'll have to reiterate on 2. and 3. until an acceptable evaluation score is achieved.\n",
    "\n",
    "Let's start by importing the useful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:28:56.097619Z",
     "start_time": "2020-08-20T09:28:53.889314Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:28:56.111827Z",
     "start_time": "2020-08-20T09:28:56.101382Z"
    }
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our model we'll be using a 5-fold cross validation with the accuracy since it's the metric that the competition uses in the leaderboard.\n",
    "\n",
    "To do that, we'll define a small scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:28:59.695041Z",
     "start_time": "2020-08-20T09:28:59.689364Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_score(clf, X, y, scoring='accuracy'):\n",
    "    xval = cross_val_score(clf, X, y, cv = 5, scoring=scoring, verbose=0)\n",
    "    return np.mean(xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recovering the train set and the test set from the combined dataset is an easy task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:29:00.587257Z",
     "start_time": "2020-08-20T09:29:00.547621Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_with_labels = pd.read_csv(\"data/new_train.csv\")\n",
    "targets = train_with_labels[\"Target\"]\n",
    "train = train_with_labels.drop(\"Target\", axis=1)\n",
    "test =  pd.read_csv(\"data/new_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:29:01.010228Z",
     "start_time": "2020-08-20T09:29:00.971202Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:29:04.625962Z",
     "start_time": "2020-08-20T09:29:04.585914Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author - Feature selection\n",
    "\n",
    "We've come up to more than 30 features so far. This number is quite large.\n",
    "\n",
    "When feature engineering is done, we usually tend to decrease the dimensionality by selecting the \"right\" number of features that capture the essential.\n",
    "\n",
    "In fact, feature selection comes with many benefits:\n",
    "\n",
    "* It decreases redundancy among the data\n",
    "* It speeds up the training process\n",
    "* It reduces overfitting\n",
    "\n",
    "Tree-based estimators can be used to compute feature importances, which in turn can be used to discard irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:29:27.410096Z",
     "start_time": "2020-08-20T09:29:27.193829Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_features='sqrt')\n",
    "clf = clf.fit(train, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the importance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:29:29.337210Z",
     "start_time": "2020-08-20T09:29:28.282082Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = pd.DataFrame()\n",
    "features['feature'] = train.columns\n",
    "features['importance'] = clf.feature_importances_\n",
    "features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
    "features.set_index('feature', inplace=True)\n",
    "\n",
    "features.plot(kind='barh', figsize=(25, 25));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may notice, there is a great importance linked to `Title_Mr`, `Age`, `Fare`, and `Sex`.\n",
    "\n",
    "There is also an important correlation with the `Passenger_Id`.\n",
    "\n",
    "Let's now transform our train set and test set in a more compact datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:29:38.251322Z",
     "start_time": "2020-08-20T09:29:38.212101Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = SelectFromModel(clf, prefit=True)\n",
    "train_reduced = model.transform(train)\n",
    "print(train_reduced.shape, end=\", \")\n",
    "\n",
    "test_reduced = model.transform(test)\n",
    "print(test_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author - First models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:29:40.646045Z",
     "start_time": "2020-08-20T09:29:40.638932Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_reduced, targets, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:29:52.713637Z",
     "start_time": "2020-08-20T09:29:41.266815Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg_cv = LogisticRegressionCV()\n",
    "rf = RandomForestClassifier()\n",
    "gboost = GradientBoostingClassifier()\n",
    "\n",
    "models = [logreg, logreg_cv, rf, gboost]\n",
    "\n",
    "for model in models:\n",
    "    np.random.seed(42)\n",
    "    print('Cross-validation of : {0}'.format(model.__class__))\n",
    "    score = compute_score(clf=model, X=X_test, y=y_test, scoring='accuracy')\n",
    "    print('CV Train score = {0}'.format(score))\n",
    "    score = compute_score(clf=model, X=X_train, y=y_train, scoring='accuracy')\n",
    "    print('CV Val score = {0}'.format(score))\n",
    "    print('****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author - Hyperparameters tuning\n",
    "\n",
    "As mentioned in the beginning of the Modeling part, we will be using a Random Forest model. It may not be the best model for this task but we'll show how to tune. This work can be applied to different models.\n",
    "\n",
    "Random Forest are quite handy. They do however come with some parameters to tweak in order to get an optimal model for the prediction task.\n",
    "\n",
    "Additionally, we'll use the full train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:29:55.976888Z",
     "start_time": "2020-08-20T09:29:55.870428Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# turn run_gs to True if you want to run the gridsearch again.\n",
    "run_gs = False\n",
    "\n",
    "if run_gs:\n",
    "    parameter_grid = {\n",
    "                 'max_depth' : [4, 6, 8],\n",
    "                 'n_estimators': [50, 10],\n",
    "                 'max_features': ['sqrt', 'auto', 'log2'],\n",
    "                 'min_samples_split': [2, 3, 10],\n",
    "                 'min_samples_leaf': [1, 3, 10],\n",
    "                 'bootstrap': [True, False],\n",
    "                 }\n",
    "    forest = RandomForestClassifier()\n",
    "    cross_validation = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    grid_search = GridSearchCV(forest,\n",
    "                               scoring='accuracy',\n",
    "                               param_grid=parameter_grid,\n",
    "                               cv=cross_validation,\n",
    "                               verbose=1\n",
    "                              )\n",
    "\n",
    "    grid_search.fit(train, targets)\n",
    "    model = grid_search\n",
    "    parameters = grid_search.best_params_\n",
    "\n",
    "    print('Best score: {}'.format(grid_search.best_score_))\n",
    "    print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "    \n",
    "else: \n",
    "    parameters = {'bootstrap': False, 'min_samples_leaf': 3, 'n_estimators': 50, \n",
    "                  'min_samples_split': 10, 'max_features': 'sqrt', 'max_depth': 6}\n",
    "    \n",
    "    model = RandomForestClassifier(**parameters)\n",
    "    model.fit(train, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is built by scanning several combinations of the hyperparameters, we can generate an output file to submit on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:30:01.442464Z",
     "start_time": "2020-08-20T09:30:01.411013Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = model.predict(test).astype(int)\n",
    "df_output = pd.DataFrame()\n",
    "aux = pd.read_csv('./data/test.csv')\n",
    "df_output['PassengerId'] = aux['PassengerId']\n",
    "df_output['Survived'] = output\n",
    "df_output[['PassengerId','Survived']].to_csv('submission/author_rand_forest.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL RESULT: 78,947% IS THE ACCURACY FOUND. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# My experiments\n",
    "\n",
    "## Validation Dataset\n",
    "\n",
    "Now, using this more updated dataset, I'll try different ways to see if I can improve the result obtained in the website as I already did before. In particular, now I have:\n",
    "* `train`: Train dataset with 67 attributes due to the pandas dummies columns\n",
    "* `targets`: Train target (Survived or no?)\n",
    "* `test`: Test dataset with 67 attributes as Train without targets (they are on Kaggle)\n",
    "\n",
    "Now I use a Validation dataset of 25% through `train_test_split()` of Scikit-Learn, obtaining:\n",
    "* `X_train,y_train`: New restricted Train dataset and its targets\n",
    "* `X_test,y_test`: New Validation dataset with its targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:30:03.056819Z",
     "start_time": "2020-08-20T09:30:03.050667Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.shape, targets.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:30:03.712251Z",
     "start_time": "2020-08-20T09:30:03.694103Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, targets, test_size=0.25)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:30:06.599019Z",
     "start_time": "2020-08-20T09:30:06.362866Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_features='sqrt')\n",
    "clf = clf.fit(train, targets)\n",
    "features = pd.DataFrame()\n",
    "features['feature'] = train.columns\n",
    "features['importance'] = clf.feature_importances_\n",
    "features.sort_values(by=['importance'], ascending=False, inplace=True)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said before, 67 attributes are too many, they make the machine learning algorithm overfit, therefore we need to implement a new solution in order to make better predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:30:07.693990Z",
     "start_time": "2020-08-20T09:30:07.686724Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features.head(13).importance.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:30:07.945932Z",
     "start_time": "2020-08-20T09:30:07.938363Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features.head(1).importance.sum(), features.head(20).importance.sum(), features.head(25).importance.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that:\n",
    "\n",
    "* `Age` and `Fare` together make the 36% of all Feature importance, in particular `Age` standalone make almost 20%;\n",
    "* The first 13/67 elements chosen in the previous case were 82% of all Feature importance;\n",
    "* 20/67 are 90%, 25/67 95%;\n",
    "\n",
    "In this case, we try 20, i.e. 90% of the feature importance. They are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:24:31.920246Z",
     "start_time": "2020-08-20T11:24:31.908859Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_list = list(features.head(20)[\"feature\"])\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:24:32.293935Z",
     "start_time": "2020-08-20T11:24:32.287075Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_train = train[feature_list] \n",
    "new_test = test[feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:24:32.630012Z",
     "start_time": "2020-08-20T11:24:32.606111Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:24:33.747765Z",
     "start_time": "2020-08-20T11:24:33.723651Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_test.reindex(np.linspace(0,418,1))\n",
    "new_test.index = pd.RangeIndex(start=0, stop=418, step=1)\n",
    "new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:24:35.769420Z",
     "start_time": "2020-08-20T11:24:35.762544Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_train, targets, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Try with different methods\n",
    "\n",
    "Now I'll try different approaches to this method, in order to have a baseline idea and work on that. The methods are:\n",
    "* `LogisticRegression()`\n",
    "* `LogisticRegressionCV()`\n",
    "* `RandomForestClassifier()`\n",
    "* `GradientBoostingClassifier()`\n",
    "\n",
    "We'll build a baseline of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:30:28.265682Z",
     "start_time": "2020-08-20T09:30:12.251104Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg_cv = LogisticRegressionCV()\n",
    "rf = RandomForestClassifier()\n",
    "gboost = GradientBoostingClassifier()\n",
    "\n",
    "models = [logreg, logreg_cv, rf, gboost]\n",
    "\n",
    "for model in models:\n",
    "    np.random.seed(42) # I need the same starting point\n",
    "    print('Cross-validation of : {0}'.format(model.__class__))\n",
    "    model.fit(X_train, y_train)\n",
    "    score = compute_score(clf=model, X=X_train, y=y_train, scoring='accuracy')\n",
    "    print('CV Train score = {0}'.format(score))\n",
    "    score = compute_score(clf=model, X=X_test, y=y_test, scoring='accuracy')\n",
    "    print('CV Validation score = {0}'.format(score))\n",
    "    print('****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As baseline result we can see that `RandomForestClassifier()` is the best at the first attempt with the new dataset, as before, even though the results obtained are slightly different than before. We'll now dive a little into the problem.\n",
    "\n",
    "## RandomizedSearchCV\n",
    "\n",
    "We'll now do a selection of the various method we proposed. Surfing the Net, we found the meaning of the main hyperparameters. In this section we try some combinations in order to understand better on which model we need to focus our attentions. We now define different grid of hyperparameters for each algorithm that later we'll apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:20:34.011418Z",
     "start_time": "2020-08-20T11:20:33.997780Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rand_grid_search(model_funct, grid, name=\"Model\", crossval=5, randomiz=True, n_iterat=1000, rand_seed=42, scoring_choose='accuracy', verb=True, X_tra=X_train, y_tra=y_train):\n",
    "    \"\"\"\n",
    "    Function that tries a model and finds its hyperparameters.\n",
    "    \"\"\"\n",
    "    np.random.seed(rand_seed) \n",
    "    if randomiz:\n",
    "        model = RandomizedSearchCV(model_funct, \n",
    "                                   param_distributions=grid,\n",
    "                                   scoring=scoring_choose,\n",
    "                                   n_iter=n_iterat,\n",
    "                                   cv=crossval,\n",
    "                                   verbose=verb, \n",
    "                                   n_jobs=-1)\n",
    "        print(name + \" found with RandomizedSearchCV\")\n",
    "    else:\n",
    "        model = GridSearchCV(model_funct, \n",
    "                             param_grid=grid,\n",
    "                             scoring=scoring_choose,\n",
    "                             cv=crossval,\n",
    "                             verbose=verb, \n",
    "                             n_jobs=-1)\n",
    "        print(name + \" found with GridSearchCV\")\n",
    "    \n",
    "    model.fit(X_tra, y_tra)\n",
    "    return model \n",
    "\n",
    "def compute_accuracy(model, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, seed=42):\n",
    "    print('*************************')\n",
    "    np.random.seed(seed)\n",
    "    print(\"Accuracy on Train Dataset\", end=\"\\t\")\n",
    "    train_acc = precision_score(y_train, model.predict(X_train))\n",
    "    print(train_acc)\n",
    "    print(\"Accuracy on Test Dataset\", end=\"\\t\")\n",
    "    test_acc = precision_score(y_test, model.predict(X_test))\n",
    "    print(test_acc)\n",
    "    return [train_acc, test_acc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:44:27.531544Z",
     "start_time": "2020-08-20T09:31:33.040584Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_randomized_logistic = False\n",
    "\n",
    "if run_randomized_logistic:\n",
    "    # Create an hyperparameter grid for Logistic Regression\n",
    "    log_reg_grid = {\"C\":np.logspace(-10, 10, 1000), \n",
    "                    \"dual\":[True, False],\n",
    "                    \"fit_intercept\":[True, False],\n",
    "                    \"penalty\" : ['none', 'l1', 'l2', 'elasticnet'],\n",
    "                    \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "    log_reg_rancv = rand_grid_search(LogisticRegression(max_iter=1000), \n",
    "                                     log_reg_grid, \n",
    "                                     name=\"Logistic Regression\", \n",
    "                                     n_iterat=10000)\n",
    "\n",
    "    accur_log_reg_rancv = compute_accuracy(log_reg_rancv)\n",
    "    \n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "    \n",
    "    pickle.dump(log_reg_rancv, open(\"model/new_version/logistic_randomizedsearch_\" + dt_string + \".pkl\", \"wb\"))\n",
    "    print('*************************')\n",
    "    print('File name')\n",
    "    print('*************************')\n",
    "    print(\"model/new_version/logistic_randomizedsearch_\" + dt_string + \".pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Result Found in:\n",
    "\n",
    "`model/new_version/logistic_randomizedsearch_20-08-2020_11-44-27.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:44:57.789433Z",
     "start_time": "2020-08-20T09:44:57.770109Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = \"model/new_version/logistic_randomizedsearch_20-08-2020_11-44-27.pkl\"\n",
    "log_reg_rancv = pickle.load(open(model_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:44:59.631068Z",
     "start_time": "2020-08-20T09:44:59.625583Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_reg_rancv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:11:13.116803Z",
     "start_time": "2020-08-20T10:05:52.210804Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_randomized_logistic_CV = False\n",
    "\n",
    "if run_randomized_logistic_CV:\n",
    "    # Create an hyperparameter grid for Logistic Regression CV \n",
    "    log_reg_CV_grid = {\"Cs\":[1, 3, 5, 7, 10, 12], \n",
    "                       \"fit_intercept\":[True, False],\n",
    "                       \"dual\":[True, False],\n",
    "                       \"penalty\" : ['l1', 'l2', 'elasticnet'],\n",
    "                       \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "    log_reg_CV_rancv = rand_grid_search(LogisticRegressionCV(max_iter=1000), \n",
    "                                        log_reg_CV_grid, \n",
    "                                        name=\"Logistic Regression CV\", \n",
    "                                        n_iterat=5000)\n",
    "\n",
    "    accur_log_reg_CV_rancv = compute_accuracy(log_reg_CV_rancv)\n",
    "    \n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "    \n",
    "    pickle.dump(log_reg_CV_rancv, open(\"model/new_version/logistic_CV_randomizedsearch_\" + dt_string + \".pkl\", \"wb\"))\n",
    "    print('*************************')\n",
    "    print('File name')\n",
    "    print('*************************')\n",
    "    print(\"model/new_version/logistic_CV_randomizedsearch_\" + dt_string + \".pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Result Found in:\n",
    "\n",
    "`model/new_version/logistic_CV_randomizedsearch_20-08-2020_12-11-13.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:11:38.121466Z",
     "start_time": "2020-08-20T10:11:38.116540Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = \"model/new_version/logistic_CV_randomizedsearch_20-08-2020_12-11-13.pkl\"\n",
    "log_reg_CV_rancv = pickle.load(open(model_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:11:39.874659Z",
     "start_time": "2020-08-20T10:11:39.869276Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_reg_CV_rancv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:16:03.669853Z",
     "start_time": "2020-08-20T10:16:03.662478Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_randomized_rand_fores = False\n",
    "\n",
    "if run_randomized_rand_fores:\n",
    "    # Create an hyperparameter grid for Random Forest Classifier\n",
    "    rf_grid = {\"n_estimators\":np.arange(10, 1000, 100), \n",
    "               \"max_depth\":[None, 3, 5, 7], \n",
    "               \"bootstrap\":[True, False],\n",
    "               \"max_features\": ['sqrt', 'auto', 'log2'],\n",
    "               \"min_samples_split\":np.arange(2, 20, 1), \n",
    "               \"min_samples_leaf\":np.arange(1, 20, 1)}\n",
    "\n",
    "    rand_fores = rand_grid_search(RandomForestClassifier(), \n",
    "                                  rf_grid, \n",
    "                                  name=\"Random Forest Classifier\", \n",
    "                                  n_iterat=500)\n",
    "\n",
    "    accur_rand_fores = compute_accuracy(rand_fores)\n",
    "    \n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "    \n",
    "    pickle.dump(rand_fores, open(\"model/new_version/rand_forest_randomizedsearch_\" + dt_string + \".pkl\", \"wb\"))\n",
    "    print('*************************')\n",
    "    print('File name')\n",
    "    print('*************************')\n",
    "    print(\"model/new_version/rand_forest_randomizedsearch_\" + dt_string + \".pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Result Found in:\n",
    "\n",
    "`model/new_version/rand_forest_randomizedsearch.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:16:55.015338Z",
     "start_time": "2020-08-20T10:16:54.978236Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = \"model/new_version/rand_forest_randomizedsearch.pkl\"\n",
    "rand_fores = pickle.load(open(model_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:16:56.108585Z",
     "start_time": "2020-08-20T10:16:56.102989Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_fores.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:19:34.187262Z",
     "start_time": "2020-08-20T10:19:34.180393Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_randomized_rs_gra_boo = False\n",
    "\n",
    "if run_randomized_rs_gra_boo:\n",
    "\n",
    "    # Create an hyperparameter grid for Gradiet Boosting Classifier\n",
    "    gb_grid = {\"loss\":['exponential', 'deviance'],\n",
    "               \"learning_rate\":[0.1, 0.05], \n",
    "               \"max_depth\":[None, 1, 3], \n",
    "               \"n_estimators\":np.arange(10, 1000, 20), \n",
    "               \"min_samples_split\":np.arange(2, 10, 1), \n",
    "               \"min_samples_leaf\":np.arange(1, 5, 1)}\n",
    "\n",
    "    rs_gra_boo = rand_grid_search(GradientBoostingClassifier(), \n",
    "                                  gb_grid, \n",
    "                                  name=\"Gradient Boosting Classifier\", \n",
    "                                  n_iterat=500)\n",
    "\n",
    "    accur_rs_gra_boo = compute_accuracy(rs_gra_boo)\n",
    "    \n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "    \n",
    "    pickle.dump(rs_gra_boo, open(\"model/new_version/grad_boost_randomizedsearch_\" + dt_string + \".pkl\", \"wb\"))\n",
    "    print('*************************')\n",
    "    print('File name')\n",
    "    print('*************************')\n",
    "    print(\"model/new_version/grad_boost_randomizedsearch_\" + dt_string + \".pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Result Found in:\n",
    "\n",
    "`model/new_version/grad_boost_randomizedsearch.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:20:29.229117Z",
     "start_time": "2020-08-20T10:20:29.212851Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = \"model/new_version/grad_boost_randomizedsearch.pkl\"\n",
    "rs_gra_boo = pickle.load(open(model_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:20:29.525496Z",
     "start_time": "2020-08-20T10:20:29.520364Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rs_gra_boo.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:20:48.666722Z",
     "start_time": "2020-08-20T10:20:48.647872Z"
    }
   },
   "outputs": [],
   "source": [
    "accur_rs_gra_boo = compute_accuracy(rs_gra_boo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between the four models we chose two of them, since they seem to behave better. \n",
    "\n",
    "Between the two logistic regression models, we select the Cross Validation one, since it gives a more realistic result even though they score the same precision.\n",
    "\n",
    "The random forest model is the best one between our approaches, since is the highest accuracy with the least overfit. \n",
    "\n",
    "Regarding the Gradient Boosting model, the overfit generated is remarkable, since the accuracy difference between Train and Validation is 12%; therefore we chose to not pursue this model.\n",
    "\n",
    "## GridSearchCV\n",
    "\n",
    "Now we'll focus, as already said, in a diver search of hyperparameters for the two chosen model, `LogisticRegressionCV()` and `RandomForestClassifier()`, through `GridSearchCV()`.\n",
    "\n",
    "We now update our grids.\n",
    "\n",
    "### Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:23:13.266692Z",
     "start_time": "2020-08-20T10:23:13.258611Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an hyperparameter grid for Logistic Regression - GridSearchCV Version\n",
    "log_reg_grid_gscv = {\"C\":np.logspace(-5, 5, 200), \n",
    "                \"dual\":[True, False],\n",
    "                \"fit_intercept\":[True, False],\n",
    "                \"penalty\" : ['l1', 'l2', 'elasticnet'],\n",
    "                \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "# Create an hyperparameter grid for Logistic Regression CV - GridSearchCV Version\n",
    "log_reg_CV_grid_gscv = {\"Cs\":[np.logspace(-1, 1, 30).tolist(), 1, 3, 5, 7, 10, 12], \n",
    "                        \"fit_intercept\":[True, False],\n",
    "                        \"dual\":[True, False],\n",
    "                        \"penalty\" : ['l1', 'l2', 'elasticnet'],\n",
    "                        \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "# Create an hyperparameter grid for Random Forest Classifier\n",
    "rf_grid_gscv = {\"n_estimators\":np.arange(10, 150, 10), \n",
    "                \"max_depth\":[4, 6, 8, 10], \n",
    "                'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                \"min_samples_split\":np.arange(6, 10, 2), \n",
    "                \"min_samples_leaf\":np.arange(6, 10, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:25:36.246144Z",
     "start_time": "2020-08-20T10:25:36.240705Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search_logistic = False\n",
    "\n",
    "if grid_search_logistic:\n",
    "\n",
    "    log_reg_gridsearch = rand_grid_search(LogisticRegression(), \n",
    "                                          log_reg_grid_gscv,\n",
    "                                          randomiz=False,  \n",
    "                                          name=\"Logistic Regression\")\n",
    "\n",
    "    accur_log_reg_rancv = compute_accuracy(log_reg_rancv)\n",
    "    \n",
    "    pickle.dump(log_reg_gridsearch, open(\"model/new_version/logistic_gridsearch.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:32:07.204663Z",
     "start_time": "2020-08-20T10:32:07.187036Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_reg_gridsearch = pickle.load(open(\"model/new_version/logistic_gridsearch.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:32:08.244777Z",
     "start_time": "2020-08-20T10:32:08.239962Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_reg_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:32:20.257750Z",
     "start_time": "2020-08-20T10:32:20.243221Z"
    }
   },
   "outputs": [],
   "source": [
    "accur_log_reg_rancv = compute_accuracy(log_reg_rancv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:30:31.642760Z",
     "start_time": "2020-08-20T10:27:44.731682Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search_logistic_CV = False\n",
    "\n",
    "if grid_search_logistic_CV:\n",
    "\n",
    "    log_reg_CV_gricv = rand_grid_search(LogisticRegressionCV(), \n",
    "                                        log_reg_CV_grid_gscv, \n",
    "                                        randomiz=False,  \n",
    "                                        name=\"Logistic Regression CV\")\n",
    "\n",
    "    accur_log_reg_CV_rancv = compute_accuracy(log_reg_CV_gricv)\n",
    "    \n",
    "    pickle.dump(log_reg_CV_gricv, open(\"model/new_version/logistic_CV_gridsearch.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:30:36.823378Z",
     "start_time": "2020-08-20T10:30:36.817405Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_reg_CV_gricv = pickle.load(open(\"model/new_version/logistic_CV_gridsearch.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:30:39.125287Z",
     "start_time": "2020-08-20T10:30:39.120027Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_reg_CV_gricv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:30:46.556705Z",
     "start_time": "2020-08-20T10:30:46.551808Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_Search_rand_fores = False\n",
    "\n",
    "if grid_Search_rand_fores:\n",
    "\n",
    "    rand_fores_gcv = rand_grid_search(RandomForestClassifier(), \n",
    "                                      rf_grid_gscv, \n",
    "                                      randomiz=False,  \n",
    "                                      name=\"Random Forest Classifier\")\n",
    "\n",
    "    accur_rand_fores_gcv = compute_accuracy(rand_fores_gcv)\n",
    "    \n",
    "    pickle.dump(rand_fores_gcv, open(\"model/new_version/rand_forest_gridsearch.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:30:47.200050Z",
     "start_time": "2020-08-20T10:30:47.184241Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_fores_gcv = pickle.load(open(\"model/new_version/rand_forest_gridsearch.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:30:48.014257Z",
     "start_time": "2020-08-20T10:30:48.008531Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_fores_gcv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T15:56:34.759307Z",
     "start_time": "2020-08-19T15:56:34.750332Z"
    }
   },
   "source": [
    "Ok, let's compare the best result obtained with the other already obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:21:52.255241Z",
     "start_time": "2020-08-20T11:21:52.056048Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {'bootstrap': False, 'min_samples_leaf': 3, 'n_estimators': 50, \n",
    "                  'min_samples_split': 10, 'max_features': 'sqrt', 'max_depth': 6}\n",
    "np.random.seed(42)\n",
    "author_model = RandomForestClassifier(**parameters)\n",
    "author_model.fit(new_train, targets)\n",
    "\n",
    "print('Author Random Forest')\n",
    "accur_rand_fores_author = compute_accuracy(author_model)\n",
    "print('Updated Random Forest')\n",
    "parameters = {'bootstrap': True, 'min_samples_leaf': 6, 'n_estimators': 30, \n",
    "                  'min_samples_split': 10, 'max_features': 'auto', 'max_depth': 10}\n",
    "anot = RandomForestClassifier(**parameters)\n",
    "anot.fit(new_train, targets)\n",
    "np.random.seed(42)\n",
    "accur_rand_fores_gcv = compute_accuracy(anot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T15:56:34.759307Z",
     "start_time": "2020-08-19T15:56:34.750332Z"
    }
   },
   "source": [
    "> Okay, We have a better result on the validation dataset!!!! \n",
    "\n",
    "Let's see what is the result on Kaggle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:55:04.002431Z",
     "start_time": "2020-08-20T10:55:03.974365Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = anot.predict(new_test).astype(int)\n",
    "df_output = pd.DataFrame()\n",
    "aux = pd.read_csv('data/test.csv')\n",
    "df_output['PassengerId'] = aux['PassengerId']\n",
    "df_output['Survived'] = output\n",
    "df_output[['PassengerId','Survived']].to_csv('submission/my_try.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:22:07.597453Z",
     "start_time": "2020-08-20T11:22:06.903012Z"
    }
   },
   "outputs": [],
   "source": [
    "print(compute_score(author_model, new_train, targets, scoring='accuracy'))\n",
    "print(compute_score(anot, new_train, targets, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T15:56:34.759307Z",
     "start_time": "2020-08-19T15:56:34.750332Z"
    }
   },
   "source": [
    "## Deeper in GridSearchCV\n",
    "\n",
    "We used already `GridSearchCV()` but we need to understand it. At the end of the process, indeed, it returns the best choiche between our attempt that score better on the train dataset we gave it. We are risking a possible overfit, hence we need to understand better any result in the better possible way. Remember: \n",
    "* `log_reg_gridsearch` is the GridSearchCV for Logistic Regression\n",
    "* `log_reg_CV_gricv` is the GridSearchCV for Logistic Regression CV\n",
    "* `rand_fores_gcv` is the GridSearchCV for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T10:37:32.369414Z",
     "start_time": "2020-08-20T10:37:32.364200Z"
    }
   },
   "outputs": [],
   "source": [
    "rand_fores_gcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:04:39.547960Z",
     "start_time": "2020-08-20T11:04:39.534522Z"
    }
   },
   "outputs": [],
   "source": [
    "df_res2 = pd.concat([pd.DataFrame(rand_fores_gcv.cv_results_[\"params\"]),\n",
    "                    pd.DataFrame(rand_fores_gcv.cv_results_[\"mean_test_score\"], columns=[\"TrainAccuracy\"])], axis=1)\n",
    "print(df_res2.shape)\n",
    "df_res = df_res2.where(pd.notnull(df_res2), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:04:40.025308Z",
     "start_time": "2020-08-20T11:04:40.009663Z"
    }
   },
   "outputs": [],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:27:37.648203Z",
     "start_time": "2020-08-20T11:27:01.456446Z"
    }
   },
   "outputs": [],
   "source": [
    "# I Want the precision also on the Validation set\n",
    "acc_test = list()\n",
    "acc_tota = list()\n",
    "count = 0\n",
    "for max_de, max_fea, min_sa_le, min_sa_spl, n_est in zip(df_res.max_depth, df_res.max_features, df_res.min_samples_leaf, df_res.min_samples_split, df_res.n_estimators):\n",
    "    parameters = {'max_depth': max_de,\n",
    "                  'max_features': max_fea,\n",
    "                  'min_samples_leaf': min_sa_le,\n",
    "                  'min_samples_split': min_sa_spl,\n",
    "                  'n_estimators': n_est\n",
    "                 }\n",
    "    np.random.seed(42)\n",
    "    last_model = RandomForestClassifier(**parameters, n_jobs=-1)\n",
    "    last_model.fit(X_train, y_train)\n",
    "    all_accuracies2 = compute_score(author_model, X_test, y_test, scoring='accuracy')\n",
    "    acc_test.append(np.mean(all_accuracies2))\n",
    "    acc_tota.append(np.mean(cross_val_score(last_model, new_train, targets, cv = 5, scoring='accuracy', verbose=0)))\n",
    "    count += 1\n",
    "    if count % 100 == 0:\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:10:33.241542Z",
     "start_time": "2020-08-20T11:10:33.234222Z"
    }
   },
   "outputs": [],
   "source": [
    "len(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:10:34.190478Z",
     "start_time": "2020-08-20T11:10:34.162134Z"
    }
   },
   "outputs": [],
   "source": [
    "df_res[\"TestAccuracy\"] = acc_test\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:10:37.888244Z",
     "start_time": "2020-08-20T11:10:37.881995Z"
    }
   },
   "outputs": [],
   "source": [
    "df_res[\"difference\"] = df_res[\"TrainAccuracy\"] - df_res[\"TestAccuracy\"]\n",
    "df_res[\"abs_difference\"] = abs(df_res[\"TrainAccuracy\"] - df_res[\"TestAccuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:10:49.057666Z",
     "start_time": "2020-08-20T11:10:49.036295Z"
    }
   },
   "outputs": [],
   "source": [
    "df_res.sort_values('TrainAccuracy', ascending=False)[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:15:32.261216Z",
     "start_time": "2020-08-20T11:15:32.244167Z"
    }
   },
   "outputs": [],
   "source": [
    "df_res2 = df_res.sort_values('TrainAccuracy', ascending=False)[0:30]\n",
    "df_res2[df_res2.abs_difference == df_res2.abs_difference.min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:15:59.737384Z",
     "start_time": "2020-08-20T11:15:59.652877Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {'max_depth': 10,\n",
    "              'max_features': 'auto',\n",
    "              'min_samples_leaf': 6,\n",
    "              'min_samples_split': 6,\n",
    "              'n_estimators': 30\n",
    "             }\n",
    "\n",
    "np.random.seed(42)\n",
    "# parameters = another.best_params_\n",
    "another = RandomForestClassifier(**parameters)\n",
    "another.fit(new_train, targets)\n",
    "print('*************************')\n",
    "print(another.score(X_train, y_train))\n",
    "print(another.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:16:36.558110Z",
     "start_time": "2020-08-20T11:16:35.837668Z"
    }
   },
   "outputs": [],
   "source": [
    "print(compute_score(author_model, new_train, targets, scoring='accuracy'))\n",
    "print(compute_score(another, new_train, targets, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T11:13:38.370534Z",
     "start_time": "2020-08-20T11:13:38.334365Z"
    }
   },
   "outputs": [],
   "source": [
    "output = another.predict(new_test).astype(int)\n",
    "df_output = pd.DataFrame()\n",
    "aux = pd.read_csv('data/test.csv')\n",
    "df_output['PassengerId'] = aux['PassengerId']\n",
    "df_output['Survived'] = output\n",
    "df_output[['PassengerId','Survived']].to_csv('submission/my_try.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
